# -*- coding: utf-8 -*-
"""Massonnet-Perrier-TP-3&4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AukW2HyXq4L8RmuyEfF8rvyMkntd3Prq
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
x_test = [[1., 1.], [0., 1.], [1., 0.], [0., 0.]]

X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Entrées

#AND
y = [0, 0, 0, 1] # Résultats attendus
clf = MLPClassifier(hidden_layer_sizes = (), activation = 'identity', solver = 'lbfgs').fit(X, y)
print("AND : ", clf.predict(x_test), "\n")

#OR
y = [0, 1, 1, 1] # Résultats attendus
clf = MLPClassifier(hidden_layer_sizes = (), activation = 'identity', solver = 'lbfgs').fit(X, y)
print("OR :  ", clf.predict(x_test), "\n")

#XOR
y = [0, 1, 1, 0] # Résultats attendus
clf = MLPClassifier(hidden_layer_sizes = (), activation = 'identity', solver = 'lbfgs').fit(X, y)
print("XOR : ", clf.predict(x_test), "\n")

"""Est-ce que les résultats prédits par le classifieur sont corrects ?
*   Non
---
Comment l’expliquez-vous ?
*   Pas assez de couche de neurones
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
x_test = [[1., 1.], [0., 1.], [1., 0.], [0., 0.]]

X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Entrées

#XOR
y = [0, 1, 1, 0] # Résultats attendus
clf = MLPClassifier(hidden_layer_sizes = (4, 2), activation = 'identity', solver = 'lbfgs').fit(X, y)
print("XOR : ", clf.predict(x_test), "\n")

"""Est-ce que les résultats prédits par le classifieur sont corrects ? 
* Non
---
Comment l’expliquez-vous ?
* La fonction d'activation n'est probablement pas adaptée. Car même en augmentant le nombre de couche de neurones, le résultat n'apparaît pas comme étant meilleur.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
x_test = [[1., 1.], [0., 1.], [1., 0.], [0., 0.]]

X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Entrées

#XOR
y = [0, 1, 1, 0] # Résultats attendus
clf = MLPClassifier(hidden_layer_sizes = (4, 2), activation = 'tanh', solver = 'lbfgs').fit(X, y)
print("XOR : ", clf.predict(x_test), "\n")

"""Est-ce que les résultats prédits par le classifieur sont corrects ?
* Parfois ; plus souvent que précédemment.
---
Comment l’expliquez-vous ?
* En utilisant un nombre de couche de neurones et une fonction d'activation paraissant plus adaptée.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.metrics import accuracy_score
dataset = load_digits()
X = dataset.data
y = dataset.target
train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.1)
clf = MLPClassifier(hidden_layer_sizes = (200, 200), activation = 'logistic', solver = 'sgd', max_iter = 2000).fit(train_X, train_y)
test_y_pred = clf.predict(test_X)
print("Accuracy : ", accuracy_score(test_y, test_y_pred))

"""Classifieurs testés (hidden_layer_size) :
* 1 : 7.22%
* 2 : 32.78% (MaxIteration : 2000 pour éviter 'ConvergenceWarning')
* 10 : 95% (MaxIteration : 2000 pour éviter 'ConvergenceWarning')
* 20 : 97.22% (MaxIteration : 2000 pour éviter 'ConvergenceWarning')
* 100 : 95%
* 200 : 97.22%
* 1000 : 96.11%
* 2000 : 97.77%
* (1, 1) : 8.33%
* (2, 2) : 10%
* (10, 10) : 91.66% (MaxIteration : 4000 pour éviter 'ConvergenceWarning')
* (20, 20) : 97.22% (MaxIteration : 3000 pour éviter 'ConvergenceWarning')
* (100, 100) : 95% (MaxIteration : 2000 pour éviter 'ConvergenceWarning')
* (200, 200) : 97.77% (MaxIteration : 2000 pour éviter 'ConvergenceWarning')
* (1, 2, 10) : 8.89%
* (10, 2, 1) : 6.11%
* (20, 10, 2) : 8.89%
* (100, 20, 10) : 8.33%
* (200, 100, 20) : 16.12%
---
Quel classifieur se comporte le mieux ? Commenter les résultats obtenus.
* Sur 1000 itérations, le meilleur MLP a une seule couche avec 2000 neurones. Cependant une couche de neurone avec 100 neurones peut également être acceptable au vu de la différence de précision.
* Sur 2000 itérations, le MLP avec une couche de 10 neuronnes est équivalent au MLP avec deux couches de 100 neuronnes. Idem pour 20 et (200, 200). Cependant, les meilleures précisions viennet des MLP 20 et (200, 200).
---
Dans tous les cas, le MLP utilise la fonction d'activation "logictic" et un solveur "sgd".
"""

import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5,), (0.5,))])

trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.MNIST(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)

classes = ('0', '1', '2', '3',
           '4', '5', '6', '7', '8', '9')



import matplotlib.pyplot as plt
import numpy as np

# functions to show an image

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# get some random training images
dataiter = iter(trainloader)
images, labels = dataiter.next()

# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        #Kernel de taille 3
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.pool = nn.MaxPool2d(2, 2)
        #Kernel de taille 3
        self.conv2 = nn.Conv2d(6, 16, 3)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()

#############################

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)


for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics28×28
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

PATH = './cifar_net.pth'
torch.save(net.state_dict(), PATH)

#############################

correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

"""LENET : Précision de 98%"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import tensorflow

mnist = tensorflow.keras.datasets.mnist
(train_X, train_y), (test_X, test_y) = mnist.load_data()

train_X = train_X.reshape(train_X.shape[0], 784)
test_X = test_X.reshape(test_X.shape[0], 784)

clf = MLPClassifier(hidden_layer_sizes = (240, 480, 120), activation = 'logistic', solver = 'sgd', max_iter = 500).fit(train_X, train_y)
test_y_pred = clf.predict(test_X)
print("Accuracy : ", accuracy_score(test_y, test_y_pred))

"""MLP : Précision de 95%

---
↓↓ Nous utilisons ce code à plusieurs reprise en modifiant certains parametres (voir en dessous) tel que les kernels et les feature maps ↓↓
---
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net2(nn.Module):
    def __init__(self):
        super(Net2, self).__init__()
        #Kernel de taill 5
        self.conv1 = nn.Conv2d(1, 16, 5)
        self.pool = nn.MaxPool2d(2, 2)
        #Kernel de taille 5
        #self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 12 * 12, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        #x = self.pool(F.relu(self.conv2(x)))
        #print(x.shape)
        #→[a, b, c, d]
        #x.view(-1, b*c*d)
        x = x.view(-1, 16*12*12)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net2 = Net2()

#############################

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net2(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics28×28
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

#############################

PATH = './cifar_net.pth'
torch.save(net2.state_dict(), PATH)

#############################

correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net2(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

"""LENET :
- Kernel → 3x3
- Feature maps → 1 / 16 / 6
- Loss final : 0.069
- Précision : 98%
---
- Kernel → 3x3
- Feature maps → 1 / 32 / 12
- Loss final : 0.057
- Précision : 98%
---
- Kernel → 5x5
- Feature maps → 1 / 6 / 16
- Loss final : 0.055
- Précision : 98%
---
- Kernel → 5x5
- Feature maps → 1 / 12 / 32
- Loss final : 0.050
- Précision : 98%
---
- Kernel → 5x5
- Feature maps → 1 / 60 / 120 (Beaucoup plus long)
- Loss final : 0.044
- Précision : 98%
---
- (Seulement deux couches linear)
- Kernel → 5x5
- Feature maps → 1 / 6 / 16
- Loss final : 0.053
- Précision : 98%
---
- (Seulement un conv2d)
- Kernel → 5x5
- Feature maps → 1 / 16
- Loss final : 0.050
- Précision : 98%
---
> À part le temps, la différence est infime.

---
Tests avec variation du learning rate
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        #Kernel de taille 3
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.pool = nn.MaxPool2d(2, 2)
        #Kernel de taille 3
        self.conv2 = nn.Conv2d(6, 16, 3)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()

#############################

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics28×28
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

#############################

PATH = './cifar_net.pth'
torch.save(net.state_dict(), PATH)

#############################

correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

"""Learning rate :
---
0.01
- loss: 1.009 (Déscent et remonte → Surentrainé)
- précision : 75%
---
0.001
- loss: 0.058
- précision : 98%
---
0.0001
- loss: 0.191 (Premier affichage à 2.294 au lieu de ~1)
- précision : 95%
---
0.00001
- loss: 2.288 (Premier affichage à 2.303)
- précision : 34%
---
> On s'aperçoit qu'avec un taux d'apprentissage élevé l'algorithme apprends plus vite, ici trop vite par rapport à l'échantillon d'entrainement. À l'inverse, lorsque le taux d'apprentissage est faible, l'algorithme apprends trop lentement par rapport à la taille de l'échantillon.
> La précision va subir des modifications en fonction.
"""